{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Standardizing Fisheries Data with LinkML","text":""},{"location":"#a-better-way-to-mobilize-marine-survey-data","title":"A Better Way to Mobilize Marine Survey Data","text":"<p>Fisheries stock assessment surveys generate valuable biodiversity data, but this information often remains difficult to access and reuse, in part becuase of reliance on closed (i.e. internal to the organization), or poorly documented standards. This project demonstrates how LinkML (Linked Data Modeling Language) provides a reusable framework can bridge the gap between domain-specific fisheries data and the internationally used biodiversity standard, Darwin Core.</p>"},{"location":"#a-solution-linkml-as-a-bridge","title":"A Solution: LinkML as a Bridge","text":"<p>The data modeling language LinkML can mitigate these challenges by creating explicit, machine-readable mappings between source data and target standards:</p> <pre><code>graph LR\n    A[The Ingredients] --&gt; B[The Recipe's&lt;br&gt;List of Ingedients]\n    B --&gt; C[The Recipe's&lt;br&gt;Instructions]\n    C --&gt; D[The Product]\n\n    style B fill:#e1f5ff\n    style C fill:#fff4e1</code></pre> <pre><code>graph LR\n    A[Source Data&lt;br/&gt;For example: ERDDAP/CSV] --&gt; B[LinkML Schema&lt;br/&gt;Data Dictionary]\n    B --&gt; C[LinkML Mappings&lt;br/&gt;to Darwin Core]\n    C --&gt; D[Darwin Core Archive&lt;br/&gt;OBIS/GBIF Ready]\n\n    style B fill:#e1f5ff\n    style C fill:#fff4e1</code></pre> <p>Key advantages:</p> <ul> <li>Transparency: Every mapping is documented and traceable</li> <li>Reusability: The same transformation engine works across different surveys</li> <li>Validation: Automated QC as part of the standardization process.</li> <li>Maintainability: The model and the code can evolve independently.</li> <li>Interoperability: LinkML schemas can map to multiple standards (e.g. Darwin Core, EML)</li> </ul>"},{"location":"#how-will-this-improve-your-science","title":"How will this improve your science?","text":"<ul> <li>Increases data discoverability: Standardized data is easy to share and easy to find. Your data become findable alongside other marine biodiversity data</li> <li>Enables synthesis: Regional interests can be more quickly assessed when researchers can combine fisheries surveys with other data sources for large-scale analyses.</li> <li>Preserves knowledge: Explicit documentation prevents loss of institutional knowledge, and provides a clear starting point for new staff.</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>View the Complete Workflow</li> <li>Explore the Architecture</li> <li>See Schema Documentation</li> <li>Learn About Reusability</li> </ul>"},{"location":"about/","title":"About This Project","text":""},{"location":"about/#overview","title":"Overview","text":"<p>This project demonstrates how LinkML (Linked Data Modeling Language) can standardize fisheries survey data for publication to biodiversity repositories. It serves as a proof of concept for mobilizing marine survey data to Darwin Core Archives.</p>"},{"location":"about/#goals","title":"Goals","text":"<ol> <li>Demonstrate reusability: Show how LinkML enables a generic transformation pattern applicable across different fisheries surveys</li> <li>Ensure traceability: Document every field transformation explicitly through machine-readable mappings</li> <li>Support interoperability: Make fisheries data discoverable alongside other marine biodiversity datasets</li> <li>Preserve knowledge: Create maintainable documentation that survives personnel changes</li> </ol>"},{"location":"about/#project-context","title":"Project Context","text":""},{"location":"about/#ocean-wind-1-ow1-fisheries-monitoring","title":"Ocean Wind 1 (OW1) Fisheries Monitoring","text":"<p>This work uses data from pre-construction fisheries surveys conducted for the Ocean Wind 1 offshore wind farm project. The surveys establish baseline conditions for fish and invertebrate communities within and around the proposed lease area.</p> <p>Survey design:</p> <ul> <li>Two seasonal surveys per year (spring and fall)</li> <li>20 tows in lease area (Impact), 20 in reference area (Control)</li> <li>Bottom otter trawl, 20 minutes at 3 knots</li> <li>Full species identification and measurement</li> </ul> <p>Data collection:</p> <ul> <li>Rutgers University Marine Field Station</li> <li>Platform: R/V Petrel</li> <li>ERDDAP data server: https://rowlrs-data.marine.rutgers.edu/erddap</li> </ul>"},{"location":"about/#why-darwin-core","title":"Why Darwin Core?","text":"<p>Darwin Core is the international standard for biodiversity data, used by:</p> <ul> <li>OBIS (Ocean Biodiversity Information System)</li> <li>GBIF (Global Biodiversity Information Facility)</li> <li>Research institutions worldwide</li> </ul> <p>Publishing fisheries data to Darwin Core:</p> <ul> <li>Increases data discoverability</li> <li>Enables large-scale synthesis</li> <li>Supports evidence-based management</li> <li>Facilitates data reuse</li> </ul>"},{"location":"about/#technical-approach","title":"Technical Approach","text":""},{"location":"about/#linkml-as-the-foundation","title":"LinkML as the Foundation","text":"<p>LinkML provides:</p> <ul> <li>Machine-readable schemas: Data models parseable by software</li> <li>Semantic mappings: Explicit relationships between source and target terms</li> <li>Validation: Type checking and constraint enforcement</li> <li>Multiple outputs: Generate documentation, code, and validation tools from one source</li> </ul>"},{"location":"about/#two-stage-transformation","title":"Two-Stage Transformation","text":"<ol> <li>Auto-rename: Generic MappingEngine handles simple 1:1 field mappings</li> <li>Custom logic: Domain-specific DwCTransformer handles complex transformations</li> </ol> <p>This separation maximizes reusability while allowing flexibility.</p>"},{"location":"about/#repository-structure","title":"Repository Structure","text":"<pre><code>IA_fisheries_trawl/\n\u251c\u2500\u2500 docs/                           # Documentation source (MkDocs)\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 workflow.md\n\u2502   \u251c\u2500\u2500 architecture/\n\u2502   \u251c\u2500\u2500 schemas/                    # Auto-generated from LinkML\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 models/                         # LinkML schemas and datasets\n\u2502   \u251c\u2500\u2500 datasets/\n\u2502   \u2502   \u2514\u2500\u2500 rutgers/               # OW1 specific implementation\n\u2502   \u2502       \u251c\u2500\u2500 ow1-catch-schema.yaml\n\u2502   \u2502       \u251c\u2500\u2500 ow1-to-dwc-mappings.yaml\n\u2502   \u2502       \u251c\u2500\u2500 ow1-to-eml-mappings.yaml\n\u2502   \u2502       \u251c\u2500\u2500 transform.py\n\u2502   \u2502       \u251c\u2500\u2500 meta.xml\n\u2502   \u2502       \u2514\u2500\u2500 ow1_dwca/          # Output files\n\u2502   \u2514\u2500\u2500 example_ideas/             # Template schemas\n\u251c\u2500\u2500 scripts/                        # Utility scripts\n\u2502   \u2514\u2500\u2500 generate_schema_docs.py    # Auto-generate docs\n\u251c\u2500\u2500 mkdocs.yml                      # Documentation config\n\u2514\u2500\u2500 .github/workflows/              # CI/CD automation\n    \u2514\u2500\u2500 deploy-docs.yml\n</code></pre>"},{"location":"about/#contributing","title":"Contributing","text":"<p>This is a proof of concept. Feedback and suggestions are welcome!</p>"},{"location":"about/#how-to-provide-feedback","title":"How to Provide Feedback","text":"<ul> <li>GitHub Issues: https://github.com/sformel/IA_fisheries_trawl/issues</li> </ul>"},{"location":"outputs/","title":"Darwin Core Archive Outputs","text":"<p>This page describes the output files generated by the transformation pipeline.</p>"},{"location":"outputs/#archive-structure","title":"Archive Structure","text":"<p>The pipeline produces a Darwin Core Archive (DwC-A) as a ZIP file containing five components:</p> <pre><code>ow1_dwca.zip\n\u251c\u2500\u2500 event.txt                           # Event core (82 records)\n\u251c\u2500\u2500 occurrence.txt                      # Occurrence extension (1,181 records)\n\u251c\u2500\u2500 extendedmeasurementorfact.txt      # eMoF extension (5,623 records)\n\u251c\u2500\u2500 meta.xml                            # Archive structure descriptor\n\u2514\u2500\u2500 eml.xml                             # Dataset metadata (EML 2.2.0)\n</code></pre> <p>Location in repository: <code>models/datasets/rutgers/ow1_dwca/</code> (unzipped) and <code>models/datasets/rutgers/ow1_dwca.zip</code></p>"},{"location":"outputs/#file-formats","title":"File Formats","text":"<p>All data files use:</p> <ul> <li>Encoding: UTF-8</li> <li>Delimiter: Tab (<code>\\t</code>)</li> <li>Line terminator: Newline (<code>\\n</code>)</li> <li>Headers: Column names in first row</li> </ul>"},{"location":"outputs/#event-core-eventtxt","title":"Event Core (event.txt)","text":"<p>Contains sampling event records with a two-level hierarchy.</p>"},{"location":"outputs/#structure","title":"Structure","text":"<p>82 total records:</p> <ul> <li>2 cruise-level parent events</li> <li>80 tow-level child events</li> </ul>"},{"location":"outputs/#record-types","title":"Record Types","text":""},{"location":"outputs/#cruise-events-parents","title":"Cruise Events (Parents)","text":"<pre><code>eventID: OW1_BT2301\nparentEventID: [empty]\neventType: cruise\neventDate: 2023-01-15T10:30:00Z\ndecimalLatitude: [empty]\ndecimalLongitude: [empty]\n...\n</code></pre> <p>Purpose: Group all tows from a single survey cruise</p>"},{"location":"outputs/#tow-events-children","title":"Tow Events (Children)","text":"<pre><code>eventID: OW1_BT2301:C01\nparentEventID: OW1_BT2301\neventType: tow\neventDate: 2023-01-15T14:22:00Z\ndecimalLatitude: 39.5123\ndecimalLongitude: -74.2045\nfootprintWKT: LINESTRING(-74.2 39.5, -74.18 39.52)\nsamplingProtocol: Bottom otter trawl\nsamplingEffort: 20 minutes at 3 knots (~1 nautical mile)\n...\n</code></pre> <p>Purpose: Represent individual trawl tows where species were collected</p>"},{"location":"outputs/#key-fields","title":"Key Fields","text":"Darwin Core Term Description Example Value eventID Unique identifier <code>OW1_BT2301:C01</code> parentEventID Links to parent cruise <code>OW1_BT2301</code> eventType Type of event <code>cruise</code> or <code>tow</code> eventDate ISO 8601 datetime <code>2023-01-15T14:22:00Z</code> locationID Station identifier <code>C01</code> decimalLatitude Midpoint latitude <code>39.5123</code> decimalLongitude Midpoint longitude <code>-74.2045</code> footprintWKT Tow track geometry <code>LINESTRING(...)</code> geodeticDatum Coordinate system <code>EPSG:4326</code> minimumDepthInMeters Min tow depth <code>15.2</code> maximumDepthInMeters Max tow depth <code>18.7</code> samplingProtocol Method used <code>Bottom otter trawl</code> sampleSizeValue Tow duration <code>20</code> sampleSizeUnit Duration units <code>minutes</code>"},{"location":"outputs/#hierarchical-relationships","title":"Hierarchical Relationships","text":"<pre><code>graph TD\n    A[Cruise: OW1_BT2301] --&gt; B[Tow: OW1_BT2301:C01]\n    A --&gt; C[Tow: OW1_BT2301:C02]\n    A --&gt; D[Tow: OW1_BT2301:C03]\n    A --&gt; E[...]\n\n    F[Cruise: OW1_BT2302] --&gt; G[Tow: OW1_BT2302:C01]\n    F --&gt; H[...]\n\n    style A fill:#e1f5ff\n    style F fill:#e1f5ff</code></pre>"},{"location":"outputs/#occurrence-extension-occurrencetxt","title":"Occurrence Extension (occurrence.txt)","text":"<p>Contains species observation records, one per species (or species + size class) caught in each tow.</p>"},{"location":"outputs/#structure_1","title":"Structure","text":"<p>1,181 total records</p> <p>Each record represents:</p> <ul> <li>A single species caught in a single tow, OR</li> <li>A single species + size class combination caught in a single tow</li> </ul>"},{"location":"outputs/#example-record","title":"Example Record","text":"<pre><code>occurrenceID: OW1_BT2301:C01:BUTTERFISH:LARGE\neventID: OW1_BT2301:C01\nbasisOfRecord: HumanObservation\noccurrenceStatus: present\nvernacularName: Butterfish\nscientificName: Peprilus triacanthus\nscientificNameID: urn:lsid:itis.gov:itis_tsn:168559\ntaxonRank: species\nkingdom: Animalia\nindividualCount: 45\noccurrenceRemarks: Size class: LARGE\n</code></pre>"},{"location":"outputs/#key-fields_1","title":"Key Fields","text":"Darwin Core Term Description Source Example occurrenceID Unique identifier Generated <code>OW1_BT2301:C01:BUTTERFISH:LARGE</code> eventID Links to tow event Generated <code>OW1_BT2301:C01</code> basisOfRecord Record type Static <code>HumanObservation</code> occurrenceStatus Presence/absence Static <code>present</code> vernacularName Common name CatchRecord <code>Butterfish</code> scientificName Scientific name SpeciesCode lookup <code>Peprilus triacanthus</code> scientificNameID ITIS LSID SpeciesCode lookup <code>urn:lsid:itis.gov:itis_tsn:168559</code> taxonRank Taxonomic level Static <code>species</code> kingdom Taxonomic kingdom Static <code>Animalia</code> individualCount Number caught CatchRecord <code>45</code> occurrenceRemarks Additional notes Generated <code>Size class: LARGE</code>"},{"location":"outputs/#taxonomic-enrichment","title":"Taxonomic Enrichment","text":"<p>Occurrence records are enriched via the species lookup table:</p> <pre><code>graph LR\n    A[CatchRecord&lt;br/&gt;species_common_name: Butterfish] --&gt; B[SpeciesCode Lookup]\n    B --&gt; C[species_scientific_name:&lt;br/&gt;Peprilus triacanthus]\n    B --&gt; D[ITIS_tsn:&lt;br/&gt;168559]\n    C --&gt; E[Occurrence&lt;br/&gt;scientificName]\n    D --&gt; F[Occurrence&lt;br/&gt;scientificNameID]\n\n    style A fill:#fff4e1\n    style E fill:#d4edda\n    style F fill:#d4edda</code></pre>"},{"location":"outputs/#extended-measurement-or-fact-extendedmeasurementorfacttxt","title":"Extended Measurement or Fact (extendedmeasurementorfact.txt)","text":"<p>Contains measurement records associated with occurrences.</p>"},{"location":"outputs/#structure_2","title":"Structure","text":"<p>5,623 total records</p> <p>Approximately 5 measurement records per occurrence:</p> <ol> <li>Size class (categorical, if present)</li> <li>Total biomass (weight in kg)</li> <li>Abundance (count of individuals)</li> <li>Mean length (mm)</li> <li>Length standard deviation (mm)</li> </ol>"},{"location":"outputs/#example-records","title":"Example Records","text":""},{"location":"outputs/#size-class-measurement","title":"Size Class Measurement","text":"<pre><code>measurementID: OW1_BT2301:C01:BUTTERFISH:LARGE_size_class\noccurrenceID: OW1_BT2301:C01:BUTTERFISH:LARGE\neventID: OW1_BT2301:C01\nmeasurementType: size class\nmeasurementValue: LARGE\nmeasurementUnit: [empty]\nmeasurementMethod: Visual assessment during sorting\nmeasurementRemarks: Categorical size class designation\n</code></pre>"},{"location":"outputs/#biomass-measurement","title":"Biomass Measurement","text":"<pre><code>measurementID: OW1_BT2301:C01:BUTTERFISH:LARGE_weight\noccurrenceID: OW1_BT2301:C01:BUTTERFISH:LARGE\neventID: OW1_BT2301:C01\nmeasurementType: total biomass\nmeasurementValue: 8.75\nmeasurementUnit: kg\nmeasurementTypeID: http://vocab.nerc.ac.uk/collection/P01/current/OWETXX01/\nmeasurementMethod: Bottom otter trawl\n</code></pre>"},{"location":"outputs/#length-measurement","title":"Length Measurement","text":"<pre><code>measurementID: OW1_BT2301:C01:BUTTERFISH:LARGE_mean_length\noccurrenceID: OW1_BT2301:C01:BUTTERFISH:LARGE\neventID: OW1_BT2301:C01\nmeasurementType: mean FL length\nmeasurementValue: 185.3\nmeasurementUnit: mm\nmeasurementTypeID: http://vocab.nerc.ac.uk/collection/P01/current/FL01XX01/\nmeasurementMethod: Caliper measurement\nmeasurementRemarks: Length type: FL\n</code></pre>"},{"location":"outputs/#key-fields_2","title":"Key Fields","text":"Darwin Core Term Description Example measurementID Unique identifier <code>OW1_BT2301:C01:BUTTERFISH_weight</code> occurrenceID Links to occurrence <code>OW1_BT2301:C01:BUTTERFISH</code> eventID Links to event <code>OW1_BT2301:C01</code> measurementType What was measured <code>total biomass</code>, <code>mean FL length</code> measurementValue Numeric or categorical value <code>8.75</code>, <code>LARGE</code> measurementUnit Units of measurement <code>kg</code>, <code>mm</code>, <code>individuals</code> measurementTypeID Controlled vocabulary URI BODC P01 or NERC codes measurementMethod How measured <code>Bottom otter trawl</code>, <code>Caliper</code> measurementRemarks Additional context <code>Length type: FL</code>"},{"location":"outputs/#measurement-types","title":"Measurement Types","text":"Type measurementType measurementUnit Count Size class <code>size class</code> [empty] ~400 Biomass <code>total biomass</code> <code>kg</code> 1,181 Abundance <code>abundance</code> <code>individuals</code> 1,181 Mean length <code>mean {type} length</code> <code>mm</code> ~1,000 Std length <code>std dev {type} length</code> <code>mm</code> ~1,000"},{"location":"outputs/#archive-descriptor-metaxml","title":"Archive Descriptor (meta.xml)","text":"<p>Defines the structure and relationships between files in the archive.</p>"},{"location":"outputs/#purpose","title":"Purpose","text":"<ul> <li>Maps file columns to Darwin Core terms</li> <li>Specifies core vs. extensions</li> <li>Defines file encoding and delimiters</li> <li>Documents foreign key relationships</li> </ul>"},{"location":"outputs/#structure_3","title":"Structure","text":"<pre><code>&lt;archive metadata=\"eml.xml\"&gt;\n  &lt;core rowType=\"http://rs.tdwg.org/dwc/terms/Event\"&gt;\n    &lt;files&gt;&lt;location&gt;event.txt&lt;/location&gt;&lt;/files&gt;\n    &lt;id index=\"0\"/&gt;\n    &lt;field index=\"0\" term=\"http://rs.tdwg.org/dwc/terms/eventID\"/&gt;\n    &lt;field index=\"1\" term=\"http://rs.tdwg.org/dwc/terms/parentEventID\"/&gt;\n    &lt;!-- ... more fields ... --&gt;\n  &lt;/core&gt;\n\n  &lt;extension rowType=\"http://rs.tdwg.org/dwc/terms/Occurrence\"&gt;\n    &lt;files&gt;&lt;location&gt;occurrence.txt&lt;/location&gt;&lt;/files&gt;\n    &lt;coreid index=\"0\"/&gt;  &lt;!-- Links to Event via eventID --&gt;\n    &lt;!-- ... fields ... --&gt;\n  &lt;/extension&gt;\n\n  &lt;extension rowType=\"http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact\"&gt;\n    &lt;files&gt;&lt;location&gt;extendedmeasurementorfact.txt&lt;/location&gt;&lt;/files&gt;\n    &lt;coreid index=\"0\"/&gt;  &lt;!-- Links to Event via eventID --&gt;\n    &lt;!-- ... fields ... --&gt;\n  &lt;/extension&gt;\n&lt;/archive&gt;\n</code></pre> <p>Location in repository: <code>models/datasets/rutgers/meta.xml</code> (template)</p>"},{"location":"outputs/#eml-metadata-emlxml","title":"EML Metadata (eml.xml)","text":"<p>Contains dataset-level metadata in Ecological Metadata Language format.</p>"},{"location":"outputs/#contents","title":"Contents","text":"<ul> <li>Dataset description: Title, abstract, purpose</li> <li>People &amp; organizations: Creators, contacts, publishers</li> <li>Project information: Funding, participants, objectives</li> <li>Coverage: Geographic, temporal, taxonomic scope</li> <li>Methods: Sampling protocols, QA/QC procedures</li> <li>Keywords: Thematic descriptors</li> <li>Intellectual rights: License and usage terms</li> </ul>"},{"location":"outputs/#structure_4","title":"Structure","text":"<pre><code>&lt;eml:eml packageId=\"bottom_trawl_survey_ow1_catch\"&gt;\n  &lt;dataset&gt;\n    &lt;title&gt;Rutgers OW1 Bottom Trawl Survey - Catch Data&lt;/title&gt;\n    &lt;creator&gt;...&lt;/creator&gt;\n    &lt;contact&gt;...&lt;/contact&gt;\n    &lt;publisher&gt;...&lt;/publisher&gt;\n    &lt;project&gt;\n      &lt;title&gt;Ocean Wind 1 Fisheries Monitoring&lt;/title&gt;\n      &lt;funding&gt;...&lt;/funding&gt;\n      &lt;personnel&gt;...&lt;/personnel&gt;\n    &lt;/project&gt;\n    &lt;abstract&gt;...&lt;/abstract&gt;\n    &lt;methods&gt;...&lt;/methods&gt;\n    &lt;coverage&gt;...&lt;/coverage&gt;\n  &lt;/dataset&gt;\n&lt;/eml:eml&gt;\n</code></pre> <p>Generated from: ERDDAP <code>NC_GLOBAL</code> attributes via <code>EMLGenerator</code></p>"},{"location":"outputs/#record-count-summary","title":"Record Count Summary","text":"Component Records Notes Source Data Tows 80 Individual trawl events Catch records 1,181 Species per tow Species codes 200+ Lookup table Darwin Core Output Events 82 2 cruises + 80 tows Occurrences 1,181 One per catch record eMoF 5,623 ~5 measurements per occurrence"},{"location":"outputs/#file-sizes","title":"File Sizes","text":"<p>Typical file sizes for this dataset:</p> File Size Rows event.txt ~15 KB 82 occurrence.txt ~120 KB 1,181 extendedmeasurementorfact.txt ~850 KB 5,623 meta.xml ~4 KB - eml.xml ~8 KB - ow1_dwca.zip ~180 KB -"},{"location":"outputs/#validation","title":"Validation","text":"<p>The archive can be validated using:</p> <ul> <li>GBIF Data Validator: https://www.gbif.org/tools/data-validator</li> <li>IPT validation: When uploading to an Integrated Publishing Toolkit instance</li> <li>OBIS QC tools: https://obis.org/manual/dataformat/</li> </ul>"},{"location":"outputs/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the Workflow</li> <li>Explore the Architecture</li> <li>Adapt for Your Data</li> </ul>"},{"location":"reusability/","title":"Reusability Guide","text":"<p>This guide explains how to adapt the LinkML-based transformation approach for your own fisheries surveys or marine biodiversity datasets.</p>"},{"location":"reusability/#the-reusable-pattern","title":"The Reusable Pattern","text":"<pre><code>flowchart LR\n    A[Your Data Source] --&gt; B[Document in LinkML]\n    B --&gt; C[Create Mapping Schema]\n    C --&gt; D[Use MappingEngine &amp; Custom Logic]\n    D --&gt; E[Darwin Core Archive]\n\n    style B fill:#e1f5ff\n    style C fill:#fff4e1\n    style D fill:#d4edda</code></pre>"},{"location":"reusability/#step-1-document-your-source-data","title":"Step 1: Document Your Source Data","text":"<p>Create a LinkML schema describing your existing data structure.</p>"},{"location":"reusability/#example-different-trawl-survey","title":"Example: Different Trawl Survey","text":"<p>Suppose you have a survey with these fields:</p> Field Name Type Description SURVEY_ID string Survey identifier HAUL_ID int Haul number DATETIME datetime Haul start time LAT_DD float Latitude (decimal degrees) LON_DD float Longitude (decimal degrees) COMMON_NAME string Species common name CATCH_WT_KG float Catch weight NUM_FISH int Number of individuals"},{"location":"reusability/#create-your-source-schema","title":"Create Your Source Schema","text":"<p>File: <code>models/datasets/your_survey/your-survey-schema.yaml</code></p> <pre><code>id: https://example.org/your-survey\nname: your-survey\ntitle: Your Survey Data Model\ndescription: Bottom trawl survey data from your institution\n\ndefault_prefix: your_survey\ndefault_range: string\n\nimports:\n  - linkml:types\n\nclasses:\n  HaulRecord:\n    description: A single trawl haul event\n    slots:\n      - survey_id\n      - haul_id\n      - datetime\n      - lat_dd\n      - lon_dd\n\n  CatchRecord:\n    description: Species catch data from a haul\n    slots:\n      - survey_id\n      - haul_id\n      - common_name\n      - catch_wt_kg\n      - num_fish\n\nslots:\n  survey_id:\n    description: Survey identifier\n    range: string\n    required: true\n    slot_uri: your_survey:survey_id\n\n  haul_id:\n    description: Haul number\n    range: integer\n    required: true\n    slot_uri: your_survey:haul_id\n\n  datetime:\n    description: Haul start time\n    range: string\n    pattern: \"^\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}Z$\"\n    slot_uri: your_survey:datetime\n\n  lat_dd:\n    description: Latitude in decimal degrees\n    range: float\n    unit:\n      ucum_code: deg\n    slot_uri: your_survey:lat_dd\n\n  lon_dd:\n    description: Longitude in decimal degrees\n    range: float\n    unit:\n      ucum_code: deg\n    slot_uri: your_survey:lon_dd\n\n  common_name:\n    description: Species common name\n    range: string\n    slot_uri: your_survey:common_name\n\n  catch_wt_kg:\n    description: Catch weight in kilograms\n    range: float\n    unit:\n      ucum_code: kg\n    minimum_value: 0.0\n    slot_uri: your_survey:catch_wt_kg\n\n  num_fish:\n    description: Number of individuals\n    range: integer\n    minimum_value: 0\n    slot_uri: your_survey:num_fish\n</code></pre> <p>Key elements:</p> <ul> <li>Classes: Organize your data into logical groups (hauls, catch, etc.)</li> <li>Slots: Define each field with type, units, constraints</li> <li>Validation: Add <code>minimum_value</code>, <code>pattern</code>, <code>required</code> as needed</li> <li>URIs: Use consistent prefix for your namespace</li> </ul>"},{"location":"reusability/#step-2-create-darwin-core-mapping-schema","title":"Step 2: Create Darwin Core Mapping Schema","text":"<p>Define how your source fields map to Darwin Core terms.</p> <p>File: <code>models/datasets/your_survey/your-survey-to-dwc.yaml</code></p> <pre><code>id: https://example.org/your-survey-dwc\nname: your-survey-dwc\ntitle: Darwin Core Mappings for Your Survey\n\ndefault_prefix: dwc\ndefault_range: string\n\nimports:\n  - linkml:types\n\nprefixes:\n  dwc: http://rs.tdwg.org/dwc/terms/\n  your_survey: https://example.org/your-survey/\n\nclasses:\n  Event:\n    description: Sampling event records\n    slots:\n      - eventID\n      - eventDate\n      - decimalLatitude\n      - decimalLongitude\n\n  Occurrence:\n    description: Species observation records\n    slots:\n      - occurrenceID\n      - eventID\n      - vernacularName\n      - individualCount\n\nslots:\n  # Event mappings\n  eventDate:\n    description: Date-time of the event\n    slot_uri: dwc:eventDate\n    exact_mappings:\n      - your_survey:datetime\n\n  decimalLatitude:\n    description: Latitude of event location\n    slot_uri: dwc:decimalLatitude\n    exact_mappings:\n      - your_survey:lat_dd\n\n  decimalLongitude:\n    description: Longitude of event location\n    slot_uri: dwc:decimalLongitude\n    exact_mappings:\n      - your_survey:lon_dd\n\n  eventID:\n    description: Unique event identifier\n    slot_uri: dwc:eventID\n    related_mappings:\n      - your_survey:survey_id\n      - your_survey:haul_id\n    comments:\n      - \"Generate as: {survey_id}:{haul_id}\"\n\n  # Occurrence mappings\n  vernacularName:\n    description: Common species name\n    slot_uri: dwc:vernacularName\n    exact_mappings:\n      - your_survey:common_name\n\n  individualCount:\n    description: Number of individuals\n    slot_uri: dwc:individualCount\n    exact_mappings:\n      - your_survey:num_fish\n\n  occurrenceID:\n    description: Unique occurrence identifier\n    slot_uri: dwc:occurrenceID\n    related_mappings:\n      - your_survey:survey_id\n      - your_survey:haul_id\n      - your_survey:common_name\n    comments:\n      - \"Generate as: {survey_id}:{haul_id}:{common_name}\"\n</code></pre> <p>Mapping types:</p> <ul> <li>exact_mappings: 1:1 field renames (auto-transformed)</li> <li>related_mappings: Requires custom logic (concatenation, calculation)</li> <li>comments: Document transformation rules</li> </ul>"},{"location":"reusability/#step-3-adapt-the-transformation-code","title":"Step 3: Adapt the Transformation Code","text":"<p>Copy and modify the transformation pipeline for your data.</p>"},{"location":"reusability/#option-a-minimal-changes","title":"Option A: Minimal Changes","text":"<p>If your data structure is similar to OW1, just update:</p> <pre><code># In your_survey_transform.py\n\n# Update dataset IDs\nDATASET_IDS = {\n    'hauls': 'your_haul_dataset_id',\n    'catch': 'your_catch_dataset_id',\n    'species': 'your_species_lookup_id'\n}\n\n# Update mapping schema path\nMAPPING_SCHEMA = \"models/datasets/your_survey/your-survey-to-dwc.yaml\"\n\n# Initialize MappingEngine with your schema\nmapping_engine = MappingEngine(MAPPING_SCHEMA)\n</code></pre>"},{"location":"reusability/#option-b-custom-transformer","title":"Option B: Custom Transformer","text":"<p>Create a new transformer class for your specific logic:</p> <pre><code>class YourSurveyTransformer:\n    \"\"\"Transform your survey data to Darwin Core.\"\"\"\n\n    def __init__(self, mapping_engine: MappingEngine):\n        self.mapping_engine = mapping_engine\n\n    def transform_to_event(self, haul_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Transform haul records to Darwin Core Events.\"\"\"\n\n        # Use mapping engine for auto-rename\n        auto_renamed = self.mapping_engine.transform_dataframe(\n            haul_df, \n            \"Event\"\n        )\n\n        # Custom logic for complex fields\n        events = []\n        for _, row in haul_df.iterrows():\n            event = {\n                'eventID': f\"{row['SURVEY_ID']}:{row['HAUL_ID']}\",\n                'eventType': 'haul',\n                # ... other custom fields\n            }\n            events.append(event)\n\n        result_df = pd.DataFrame(events)\n\n        # Merge auto-renamed fields\n        for col in auto_renamed.columns:\n            if col not in result_df.columns:\n                result_df[col] = auto_renamed[col]\n\n        return result_df\n</code></pre>"},{"location":"reusability/#step-4-generate-documentation","title":"Step 4: Generate Documentation","text":"<p>Update the documentation generator configuration:</p> <p>In <code>scripts/generate_schema_docs.py</code>:</p> <pre><code>SCHEMAS = {\n    'your-survey-schema.yaml': {\n        'output': 'your-survey-source.md',\n        'title': 'Your Survey Source Data',\n        'type': 'source'\n    },\n    'your-survey-to-dwc.yaml': {\n        'output': 'your-survey-dwc.md',\n        'title': 'Your Survey Darwin Core Mappings',\n        'type': 'mappings'\n    }\n}\n</code></pre> <p>Then generate docs:</p> <pre><code>python scripts/generate_schema_docs.py\n</code></pre>"},{"location":"reusability/#step-5-test-and-validate","title":"Step 5: Test and Validate","text":""},{"location":"reusability/#test-locally","title":"Test Locally","text":"<pre><code># Run transformation\npython models/datasets/your_survey/transform.py\n\n# Check outputs\nls models/datasets/your_survey/dwca/\n\n# Validate with GBIF validator\n# Upload to: https://www.gbif.org/tools/data-validator\n</code></pre>"},{"location":"reusability/#common-issues","title":"Common Issues","text":"<p>Problem: MappingEngine skips fields Solution: Check that <code>exact_mappings</code> has exactly one source</p> <p>Problem: Missing required Darwin Core fields Solution: Add custom logic in transformer for complex fields</p> <p>Problem: Type conversion errors Solution: Verify <code>range</code> in mapping schema matches source data type</p>"},{"location":"reusability/#reusable-components","title":"Reusable Components","text":"<p>These components work across any dataset:</p>"},{"location":"reusability/#mappingengine-zero-changes-needed","title":"MappingEngine (Zero Changes Needed)","text":"<pre><code># Works with any LinkML mapping schema\nengine = MappingEngine('path/to/mappings.yaml')\ndf = engine.transform_dataframe(source_df, 'TargetClass')\n</code></pre>"},{"location":"reusability/#documentation-generator-minimal-config","title":"Documentation Generator (Minimal Config)","text":"<pre><code># Just update SCHEMAS dict\nSCHEMAS = {'your-schema.yaml': {...}}\n</code></pre>"},{"location":"reusability/#metaxml-template-reusable","title":"Meta.xml Template (Reusable)","text":"<p>Copy <code>models/datasets/rutgers/meta.xml</code> - it's generic!</p>"},{"location":"reusability/#what-to-customize","title":"What to Customize","text":"<p>For each new dataset, you'll need to:</p> <ol> <li>Source schema (100% custom) - Document your data structure</li> <li>Mapping schema (80% similar) - Map to Darwin Core (structure is reusable)</li> <li>Custom transformer (50% similar) - Adapt ID generation and complex logic</li> <li>Documentation config (minimal) - Update file paths</li> </ol>"},{"location":"reusability/#getting-help","title":"Getting Help","text":""},{"location":"reusability/#linkml-resources","title":"LinkML Resources","text":"<ul> <li>Documentation: https://linkml.io/</li> <li>Tutorials: https://linkml.io/linkml/intro/tutorial.html</li> <li>Slack: LinkML community workspace</li> </ul>"},{"location":"reusability/#darwin-core-resources","title":"Darwin Core Resources","text":"<ul> <li>Standard: https://dwc.tdwg.org/</li> <li>GBIF Guide: https://www.gbif.org/darwin-core</li> <li>OBIS Manual: https://obis.org/manual/</li> </ul>"},{"location":"reusability/#this-project","title":"This Project","text":"<ul> <li>GitHub Issues: https://github.com/sformel/IA_fisheries_trawl/issues</li> <li>Email: steve@formeldataservices.com</li> </ul> <p>Ready to start? Begin with Step 1: Document Your Source Data</p> <p>Need examples? See the complete OW1 implementation</p>"},{"location":"workflow/","title":"Complete Workflow","text":"<p>This page walks through the entire data transformation pipeline from source fisheries data to a standards-compliant Darwin Core Archive.</p>"},{"location":"workflow/#overview-diagram","title":"Overview Diagram","text":"<pre><code>graph LR\n    A[Source Data&lt;br/&gt;For example: ERDDAP/CSV] --&gt; B[LinkML Schema&lt;br/&gt;Data Dictionary]\n    B --&gt; C[LinkML Mappings&lt;br/&gt;to Darwin Core]\n    C --&gt; D[Darwin Core Archive&lt;br/&gt;OBIS/GBIF Ready]\n\n    style B fill:#e1f5ff\n    style C fill:#fff4e1</code></pre>"},{"location":"workflow/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"workflow/#1-source-data-erddap-datasets","title":"1. Source Data: ERDDAP Datasets","text":"<p>https://rowlrs-data.marine.rutgers.edu/erddap</p> <p>The workflow begins with three datasets hosted on a Rutgers ERDDAP server:</p> <ul> <li>bottom_trawl_survey_ow1_tows: Station/tow metadata (time, location, coordinates)</li> <li>bottom_trawl_survey_ow1_catch: Species catch data (weights, counts, lengths)</li> <li>species_id_codes: Taxonomic lookup (common names \u2192 scientific names \u2192 ITIS TSNs)</li> </ul> <p>Challenge: These datasets use fisheries-specific terminology and structure that doesn't directly match Darwin Core.</p>"},{"location":"workflow/#2-make-linkml-schema-aka-data-model-aka-data-dictionary","title":"2. Make LinkML Schema (AKA data model, AKA data dictionary)","text":"<p>Here we are describing two parts: what 'terms' do we have, and how should those terms be grouped? </p> <p>Note: if you already have a SQL schema, there are tools to automatically turn this into LinkML.</p>"},{"location":"workflow/#what-terms-do-we-have","title":"What terms do we have?","text":"<p>Again, this is essentially a well-structured data dictionary. Terms might look like this:</p> <pre><code>latitude:\n    description: Latitude coordinates entered at start of trawl\n    range: float\n    required: true\n    unit:\n      ucum_code: deg\n    slot_uri: ow1_catch:latitude\n    annotations:\n      erddap_source: \"latitude (start_latitude)\"\n      erddap_standard_name: \"latitude\"\n      erddap_units: \"degrees_north\"\n</code></pre>"},{"location":"workflow/#how-should-those-terms-be-grouped","title":"How should those terms be grouped?","text":"<p>These are like tables in a database. We define classes, and there is nothing wrong with a term being used in multiple classes.</p> <pre><code>classes:\n  TowRecord:\n    description: A single trawl tow/station event\n    slots:\n      - cruise\n      - station\n      - time\n      - latitude\n      - longitude\n      - end_latitude\n      - end_longitude\n</code></pre>"},{"location":"workflow/#why-is-this-a-good-approach","title":"Why is this a good approach?","text":"<ul> <li>Machine-readable data dictionary</li> <li>Document units and sources</li> <li>directs data validation for QC (e.g. are all dates actually dates?)</li> </ul>"},{"location":"workflow/#3-link-mappings-to-darwin-core","title":"3. Link Mappings to Darwin Core","text":"<p>How does Darwin Core compare to our model? Here is an example of the Darwin Core term <code>eventDate</code>, which happens to match our trawl term, <code>time</code>, exactly. No additional context is needed.</p> <pre><code>slots:\n  eventDate:\n    description: The date-time during which an Event occurred\n    slot_uri: dwc:eventDate\n    exact_mappings:\n      - ow1_catch:time  # Shows source field\n</code></pre> <p>But what about when they don't match up exactly?  Here we began to decide what we will do. Importantly, we're capturing the information in a structured way so it can be leveraged later.</p> <pre><code>  decimalLongitude:\n    description: &gt;-\n      The geographic longitude in decimal degrees of the geographic center of a Location.\n    range: float\n    slot_uri: dwc:decimalLongitude\n    unit:\n      ucum_code: deg\n    related_mappings:\n      - ow1_catch:start_longitude\n      - ow1_catch:end_longitude\n    comments:\n      - \"Calculate midpoint from TowRecord.start_longitude and TowRecord.end_longitude\"\n      - \"Or use start_longitude as representative point\"\n</code></pre>"},{"location":"workflow/#4-create-a-darwin-core-archive-ready-for-obisgbif","title":"4. Create a Darwin Core Archive: Ready for OBIS/GBIF","text":"<p>Here we have python code to take our LinkML instructions and create a valid Darwin Core Archive for sharing to OBIS and GBIF.</p> <p>What it handles automatically:</p> <p>Some things we can easily reuse across data sources</p> <ul> <li>Simple 1:1 field renames (e.g., <code>time</code> \u2192 <code>eventDate</code>)</li> <li>Type conversions (string, float, integer)</li> <li>Validation against target schema</li> </ul> <p>What requires custom logic:</p> <p>Some things will require careful consideration of how that particular dataset works</p> <ul> <li>Complex field generation (IDs, hierarchies)</li> <li>Calculated fields (midpoints, WKT geometries)</li> <li>Multi-source field merging</li> </ul> <p>The bottom line</p> <p>As much as possible we're using templates and generic functions to make this reusable and quickly implemented for a fisheries dataset.</p>"},{"location":"workflow/#the-darwin-core-archive-output","title":"The Darwin Core Archive Output","text":"<p>The pipeline produces a ZIP file containing:</p> <pre><code>ow1_dwca.zip\n\u251c\u2500\u2500 event.txt                           # 82 records (2 cruises + 80 tows)\n\u251c\u2500\u2500 occurrence.txt                      # 1,181 species observations\n\u251c\u2500\u2500 extendedmeasurementorfact.txt      # 5,623 measurements\n\u251c\u2500\u2500 meta.xml                            # Archive structure descriptor\n\u2514\u2500\u2500 eml.xml                             # Dataset metadata\n</code></pre>"},{"location":"workflow/#publication-to-obisgbif","title":"Publication to OBIS/GBIF","text":"<p>The resulting archive can be:</p> <ul> <li>Uploaded directly to OBIS/GBIF Integrated Publishing Toolkit (IPT)</li> <li>Validated using GBIF Data Validator</li> <li>Published to make data globally discoverable</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>This page describes the technical architecture of the LinkML-based transformation pipeline.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Data Layer\"\n    A[ERDDAP Server]\n    B[Source Datasets]\n    end\n\n    subgraph \"Schema Layer\"\n    C[Source Schema&lt;br/&gt;ow1-catch-schema.yaml]\n    D[DwC Mapping Schema&lt;br/&gt;ow1-to-dwc-mappings.yaml]\n    E[EML Mapping Schema&lt;br/&gt;ow1-to-eml-mappings.yaml]\n    end\n\n    subgraph \"Transformation Layer\"\n    F[MappingEngine&lt;br/&gt;Generic auto-rename]\n    G[DwCTransformer&lt;br/&gt;Business logic]\n    H[EMLGenerator&lt;br/&gt;Metadata]\n    end\n\n    subgraph \"Output Layer\"\n    I[DwC Archive Writer]\n    J[Event Core]\n    K[Occurrence Extension]\n    L[eMoF Extension]\n    M[meta.xml]\n    N[eml.xml]\n    end\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    C --&gt; E\n    D --&gt; F\n    F --&gt; G\n    G --&gt; J\n    G --&gt; K\n    G --&gt; L\n    E --&gt; H\n    H --&gt; N\n    M --&gt; I\n    J --&gt; I\n    K --&gt; I\n    L --&gt; I\n    N --&gt; I\n\n    I --&gt; O[Darwin Core Archive ZIP]\n\n    style C fill:#e1f5ff\n    style D fill:#fff4e1\n    style E fill:#fff4e1\n    style F fill:#d4edda</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-schema-layer-linkml","title":"1. Schema Layer (LinkML)","text":"<p>The foundation of the system is three LinkML YAML schemas:</p>"},{"location":"architecture/overview/#source-data-schema","title":"Source Data Schema","text":"<p>File: <code>ow1-catch-schema.yaml</code></p> <p>Purpose: Documents the existing fisheries data structure</p> <p>Contents:</p> <ul> <li>Class definitions for each dataset (TowRecord, CatchRecord, SpeciesCode)</li> <li>Field definitions with types, units, and descriptions</li> <li>Annotations linking to ERDDAP sources</li> <li>Dataset-level metadata (creator, publisher, funding, etc.)</li> </ul> <p>Example:</p> <pre><code>slots:\n  total_weight:\n    description: Total weight in kilograms of measured individuals\n    range: float\n    unit:\n      ucum_code: kg\n    annotations:\n      erddap_source: \"total_weight\"\n      erddap_units: \"kg\"\n</code></pre>"},{"location":"architecture/overview/#darwin-core-mapping-schema","title":"Darwin Core Mapping Schema","text":"<p>File: <code>ow1-to-dwc-mappings.yaml</code></p> <p>Purpose: Defines the target Darwin Core structure with mappings to source</p> <p>Contents:</p> <ul> <li>Darwin Core class definitions (Event, Occurrence, ExtendedMeasurementOrFact)</li> <li>Field definitions with Darwin Core term URIs</li> <li><code>exact_mappings</code> showing source \u2192 target relationships</li> <li>Comments explaining complex transformations</li> </ul> <p>Example:</p> <pre><code>slots:\n  eventDate:\n    description: The date-time during which an Event occurred\n    range: string\n    slot_uri: dwc:eventDate\n    exact_mappings:\n      - ow1_catch:time\n    comments:\n      - \"Direct mapping from TowRecord.time\"\n      - \"Format as ISO 8601 datetime\"\n</code></pre>"},{"location":"architecture/overview/#eml-mapping-schema","title":"EML Mapping Schema","text":"<p>File: <code>ow1-to-eml-mappings.yaml</code></p> <p>Purpose: Maps dataset metadata to EML structure</p> <p>Contents:</p> <ul> <li>EML element definitions</li> <li>Mappings to source metadata fields</li> <li>Instructions for metadata transformation</li> </ul>"},{"location":"architecture/overview/#2-transformation-layer-python","title":"2. Transformation Layer (Python)","text":""},{"location":"architecture/overview/#mappingengine-generic","title":"MappingEngine (Generic)","text":"<p>Purpose: Dataset-agnostic transformation based on LinkML mappings</p> <p>Key methods:</p> <pre><code>class MappingEngine:\n    def __init__(self, mapping_schema_path):\n        # Load LinkML schema\n        self.schema = self._load_schema()\n\n    def transform_dataframe(self, source_df, target_class, strict=True):\n        # Auto-rename fields based on exact_mappings\n        # Only processes 1:1 mappings when strict=True\n        # Handles type conversion\n        return transformed_df\n</code></pre> <p>Transformation rules:</p> <ul> <li>Only processes fields with exactly one <code>exact_mapping</code></li> <li>Performs type conversion based on LinkML <code>range</code></li> <li>Skips fields requiring complex logic</li> <li>Issues warnings for missing required fields</li> </ul> <p>Reusability: This engine works with any dataset following the same LinkML pattern.</p>"},{"location":"architecture/overview/#dwctransformer-domain-specific","title":"DwCTransformer (Domain-Specific)","text":"<p>Purpose: Business logic for complex transformations that can't be auto-mapped</p> <p>Handles:</p> <ul> <li>ID generation: Creating unique eventID, occurrenceID, measurementID</li> <li>Hierarchical structures: Parent-child event relationships</li> <li>Calculated fields: Midpoint coordinates, WKT geometries</li> <li>Multi-source enrichment: Joining catch data with species lookup</li> <li>Splitting records: Generating multiple eMoF records per catch record</li> </ul> <p>Example of custom logic:</p> <pre><code>def create_event_id(self, cruise: str, station: str) -&gt; str:\n    \"\"\"Generate DwC eventID from cruise and station.\"\"\"\n    return f\"{cruise}:{station}\"\n\ndef calculate_midpoint(self, start_lat, start_lon, end_lat, end_lon):\n    \"\"\"Calculate geographic midpoint of tow.\"\"\"\n    return (start_lat + end_lat) / 2, (start_lon + end_lon) / 2\n</code></pre>"},{"location":"architecture/overview/#emlgenerator","title":"EMLGenerator","text":"<p>Purpose: Generate EML metadata from ERDDAP attributes</p> <p>Process:</p> <ol> <li>Fetch <code>NC_GLOBAL</code> metadata from ERDDAP info endpoint</li> <li>Parse structured attributes (contributors, keywords)</li> <li>Map to EML elements using schema</li> <li>Generate valid EML 2.2.0 XML</li> </ol>"},{"location":"architecture/overview/#3-output-layer","title":"3. Output Layer","text":""},{"location":"architecture/overview/#dwcarchivewriter","title":"DwCArchiveWriter","text":"<p>Purpose: Write Darwin Core Archive files and package as ZIP</p> <p>Functions:</p> <ul> <li>Write tab-delimited text files (UTF-8)</li> <li>Copy meta.xml template</li> <li>Write EML XML</li> <li>Create ZIP archive</li> </ul> <p>File structure:</p> <pre><code>ow1_dwca.zip\n\u251c\u2500\u2500 event.txt               # Tab-delimited, UTF-8\n\u251c\u2500\u2500 occurrence.txt          # Tab-delimited, UTF-8  \n\u251c\u2500\u2500 extendedmeasurementorfact.txt\n\u251c\u2500\u2500 meta.xml               # Archive descriptor\n\u2514\u2500\u2500 eml.xml                # Dataset metadata\n</code></pre>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#hybrid-transformation-approach","title":"Hybrid Transformation Approach","text":"<p>The system uses two complementary strategies:</p> <ol> <li>Auto-rename via MappingEngine: For simple 1:1 field mappings</li> <li>Custom logic via DwCTransformer: For complex transformations</li> </ol> <pre><code># In DwCTransformer.transform_to_occurrence():\n\n# Step 1: Auto-rename simple fields\nauto_renamed = self.mapping_engine.transform_dataframe(merged, \"Occurrence\")\n\n# Step 2: Custom logic for complex fields\nfor _, row in merged.iterrows():\n    occurrence = {\n        'occurrenceID': self.create_occurrence_id(...),  # Custom\n        'eventID': self.create_event_id(...),            # Custom\n        'basisOfRecord': 'HumanObservation',             # Static\n        'scientificNameID': self.format_itis_lsid(...),  # Custom\n        # ... other fields\n    }\n\n# Step 3: Merge auto-renamed with custom\nresult_df = pd.DataFrame(occurrences)\nfor col in auto_renamed.columns:\n    if col not in result_df.columns:\n        result_df[col] = auto_renamed[col]\n</code></pre> <p>Rationale: </p> <ul> <li>Maximizes reusability (auto-rename works across datasets)</li> <li>Allows flexibility (custom logic when needed)</li> <li>Clear separation of concerns</li> </ul>"},{"location":"architecture/overview/#strict-mapping-policy","title":"Strict Mapping Policy","text":"<p>The MappingEngine enforces strict 1:1 mappings:</p> <pre><code>if len(exact_mappings) != 1:\n    if strict:\n        continue  # Skip this field\n</code></pre> <p>Why?</p> <ul> <li>Unambiguous transformations: One source field maps to exactly one target field</li> <li>Prevents errors: No guessing about which source field to use</li> <li>Clear documentation: Each mapping is explicit in the schema</li> </ul> <p>Complex cases (multiple sources, calculations) must use custom logic in DwCTransformer.</p>"},{"location":"architecture/overview/#template-based-archive-structure","title":"Template-Based Archive Structure","text":"<p>The <code>meta.xml</code> file is a static template, not generated code:</p> <pre><code>&lt;archive xmlns=\"http://rs.tdwg.org/dwc/text/\" metadata=\"eml.xml\"&gt;\n  &lt;core encoding=\"UTF-8\" fieldsTerminatedBy=\"\\t\" \n        rowType=\"http://rs.tdwg.org/dwc/terms/Event\"&gt;\n    &lt;files&gt;\n      &lt;location&gt;event.txt&lt;/location&gt;\n    &lt;/files&gt;\n    &lt;!-- field mappings --&gt;\n  &lt;/core&gt;\n  &lt;!-- extensions --&gt;\n&lt;/archive&gt;\n</code></pre> <p>Benefits:</p> <ul> <li>Easy to modify archive structure without code changes</li> <li>Can swap in different templates for different publication formats</li> <li>Clear separation between data transformation and archive packaging</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant E as ERDDAP\n    participant Ex as ERDDAPExtractor\n    participant ME as MappingEngine\n    participant DT as DwCTransformer\n    participant W as DwCArchiveWriter\n\n    Ex-&gt;&gt;E: Fetch datasets (CSV)\n    E--&gt;&gt;Ex: Return DataFrames\n\n    Ex-&gt;&gt;DT: Pass source data\n\n    DT-&gt;&gt;ME: Auto-rename simple fields\n    ME--&gt;&gt;DT: Transformed DataFrame\n\n    DT-&gt;&gt;DT: Apply custom logic\n    DT-&gt;&gt;DT: Generate IDs, hierarchies\n    DT-&gt;&gt;DT: Calculate geometries\n\n    DT--&gt;&gt;W: Event DataFrame\n    DT--&gt;&gt;W: Occurrence DataFrame\n    DT--&gt;&gt;W: eMoF DataFrame\n\n    W-&gt;&gt;W: Write tab-delimited files\n    W-&gt;&gt;W: Copy meta.xml template\n    W-&gt;&gt;W: Generate eml.xml\n    W-&gt;&gt;W: Create ZIP archive\n\n    W--&gt;&gt;E: Darwin Core Archive</code></pre>"},{"location":"architecture/overview/#technology-stack","title":"Technology Stack","text":"Component Technology Purpose Schema Definition LinkML YAML Machine-readable data models Schema Validation LinkML Python Runtime validation Data Extraction Requests + Pandas Fetch from ERDDAP, parse CSV Transformation Python + Pandas Data manipulation Metadata XML generation EML 2.2.0 output Archive zipfile Package as DwC-A Documentation MkDocs + LinkML gen-doc Human-readable docs"},{"location":"architecture/overview/#extensibility-points","title":"Extensibility Points","text":"<p>The architecture supports several extension paths:</p> <ol> <li>New source formats: Add extractors for different data sources (databases, APIs, CSV files)</li> <li>Additional target standards: Create new mapping schemas (e.g., ABCD, MIDS, FAIR Data Point)</li> <li>Validation rules: Add LinkML constraints and validation logic</li> <li>Quality control: Insert QC steps between extraction and transformation</li> <li>Alternative outputs: Generate additional formats (JSON-LD, RDF, Parquet)</li> </ol> <p>Next: Data Models | Transformation Engine</p>"},{"location":"architecture/transformation_engine/","title":"Transformation Engine","text":"<p>The transformation engine is the core component that converts source data to Darwin Core format using LinkML mappings.</p>"},{"location":"architecture/transformation_engine/#two-stage-transformation","title":"Two-Stage Transformation","text":"<pre><code>flowchart TD\n    A[Source DataFrame] --&gt; B{MappingEngine}\n    B --&gt;|exact_mappings| C[Auto-Renamed Fields]\n    A --&gt; D{DwCTransformer}\n    D --&gt;|Custom Logic| E[Complex Fields]\n    C --&gt; F[Merge]\n    E --&gt; F\n    F --&gt; G[Darwin Core DataFrame]\n\n    style B fill:#d4edda\n    style D fill:#fff3cd</code></pre>"},{"location":"architecture/transformation_engine/#stage-1-mappingengine-generic","title":"Stage 1: MappingEngine (Generic)","text":"<p>The <code>MappingEngine</code> is a dataset-agnostic component that works with any LinkML mapping schema.</p>"},{"location":"architecture/transformation_engine/#initialization","title":"Initialization","text":"<pre><code>class MappingEngine:\n    def __init__(self, mapping_schema_path: str):\n        # Load LinkML schema YAML\n        self.schema = self._load_schema()\n        self.classes = self.schema.get('classes', {})\n        self.slots = self.schema.get('slots', {})\n</code></pre> <p>Loads and parses the mapping schema to understand:</p> <ul> <li>Which classes exist (Event, Occurrence, etc.)</li> <li>Which slots each class has</li> <li>What mappings are defined for each slot</li> </ul>"},{"location":"architecture/transformation_engine/#core-algorithm","title":"Core Algorithm","text":"<pre><code>def transform_dataframe(self, source_df, target_class, strict=True):\n    mappings = self._get_slot_mappings(target_class)\n    result = pd.DataFrame()\n\n    for target_field, mapping_spec in mappings.items():\n        exact_mappings = mapping_spec['exact_mappings']\n\n        # STRICT RULE: Only process 1:1 mappings\n        if len(exact_mappings) != 1:\n            continue  # Skip this field\n\n        # Extract source field name\n        source_field = self._extract_source_field(exact_mappings[0])\n\n        # Check if source field exists\n        if source_field not in source_df.columns:\n            continue\n\n        # Copy and convert the column\n        target_range = mapping_spec['range']\n        result[target_field] = source_df[source_field].apply(\n            lambda x: self._convert_type(x, target_range)\n        )\n\n    return result\n</code></pre>"},{"location":"architecture/transformation_engine/#processing-rules","title":"Processing Rules","text":"<p>1. Strict 1:1 Mapping</p> <p>Only fields with exactly one <code>exact_mapping</code> are processed:</p> <pre><code># \u2705 WILL be auto-transformed\nvernacularName:\n  exact_mappings:\n    - ow1_catch:species_common_name\n\n# \u274c WILL NOT be auto-transformed (multiple mappings)\ndecimalLatitude:\n  related_mappings:\n    - ow1_catch:latitude\n    - ow1_catch:end_latitude\n\n# \u274c WILL NOT be auto-transformed (no mappings)\noccurrenceID:\n  # (requires custom ID generation)\n</code></pre> <p>2. Type Conversion</p> <p>Automatically converts based on LinkML <code>range</code>:</p> LinkML Range Python Type Conversion <code>string</code> <code>str</code> <code>str(value)</code> <code>float</code> <code>float</code> <code>float(value)</code> <code>integer</code> <code>int</code> <code>int(value)</code> <code>boolean</code> <code>bool</code> <code>bool(value)</code> <p>3. Field Name Extraction</p> <p>Handles prefixed mappings:</p> <pre><code>def _extract_source_field(self, mapping: str) -&gt; str:\n    # \"ow1_catch:species_common_name\" \u2192 \"species_common_name\"\n    if ':' in mapping:\n        return mapping.split(':', 1)[1]\n    return mapping\n</code></pre>"},{"location":"architecture/transformation_engine/#example-occurrence-transformation","title":"Example: Occurrence Transformation","text":"<p>Input mapping schema:</p> <pre><code>slots:\n  vernacularName:\n    slot_uri: dwc:vernacularName\n    range: string\n    exact_mappings:\n      - ow1_catch:species_common_name\n\n  scientificName:\n    slot_uri: dwc:scientificName\n    range: string\n    exact_mappings:\n      - ow1_catch:species_scientific_name\n</code></pre> <p>Input DataFrame:</p> species_common_name species_scientific_name total_count Butterfish Peprilus triacanthus 45 Summer Flounder Paralichthys dentatus 12 <p>MappingEngine output:</p> vernacularName scientificName Butterfish Peprilus triacanthus Summer Flounder Paralichthys dentatus <p>Note: <code>total_count</code> is not in the output because it has no <code>exact_mapping</code> in the mapping schema.</p>"},{"location":"architecture/transformation_engine/#stage-2-dwctransformer-custom-logic","title":"Stage 2: DwCTransformer (Custom Logic)","text":"<p>The <code>DwCTransformer</code> handles transformations that cannot be auto-mapped.</p>"},{"location":"architecture/transformation_engine/#id-generation","title":"ID Generation","text":"<p>Challenge: Darwin Core requires unique identifiers that don't exist in source data.</p> <p>Solution: Generate structured IDs from source fields:</p> <pre><code>@staticmethod\ndef create_event_id(cruise: str, station: str) -&gt; str:\n    \"\"\"Generate DwC eventID from cruise and station.\"\"\"\n    return f\"{cruise}:{station}\"\n    # Example: \"OW1_BT2301:C01\"\n\n@staticmethod\ndef create_occurrence_id(cruise: str, station: str, \n                        species: str, size_class: str = None) -&gt; str:\n    \"\"\"Generate DwC occurrenceID, including size_class if present.\"\"\"\n    species_code = str(species).replace(' ', '_').replace('/', '_')\n    base_id = f\"{cruise}:{station}:{species_code}\"\n    if size_class and pd.notna(size_class):\n        size_code = str(size_class).replace(' ', '_').upper()\n        return f\"{base_id}:{size_code}\"\n    return base_id\n    # Example: \"OW1_BT2301:C01:BUTTERFISH:LARGE\"\n</code></pre>"},{"location":"architecture/transformation_engine/#hierarchical-events","title":"Hierarchical Events","text":"<p>Challenge: Create parent-child event relationships (cruise \u2192 tow).</p> <p>Solution: Two-pass event generation:</p> <pre><code>def transform_to_event(self, tow_df: pd.DataFrame) -&gt; pd.DataFrame:\n    events = []\n\n    # Pass 1: Create cruise-level parent events\n    cruises = tow_df.groupby('cruise').agg({\n        'time': 'min'  # Use earliest tow time\n    }).reset_index()\n\n    for _, cruise_row in cruises.iterrows():\n        cruise_event = {\n            'eventID': cruise_row['cruise'],\n            'parentEventID': None,  # No parent\n            'eventType': 'cruise',\n            'eventDate': cruise_row['time'],\n            # ...\n        }\n        events.append(cruise_event)\n\n    # Pass 2: Create tow-level child events\n    for _, tow_row in tow_df.iterrows():\n        tow_event = {\n            'eventID': self.create_event_id(tow_row['cruise'], tow_row['station']),\n            'parentEventID': tow_row['cruise'],  # Links to parent\n            'eventType': 'tow',\n            'eventDate': tow_row['time'],\n            # ...\n        }\n        events.append(tow_event)\n\n    return pd.DataFrame(events)\n</code></pre>"},{"location":"architecture/transformation_engine/#calculated-fields","title":"Calculated Fields","text":"<p>Challenge: Create fields from multiple sources or calculations.</p>"},{"location":"architecture/transformation_engine/#geographic-midpoint","title":"Geographic Midpoint","text":"<pre><code>@staticmethod\ndef calculate_midpoint(start_lat, start_lon, end_lat, end_lon):\n    \"\"\"Calculate geographic midpoint of tow.\"\"\"\n    return (start_lat + end_lat) / 2, (start_lon + end_lon) / 2\n\n# Usage in transform_to_event():\nmid_lat, mid_lon = self.calculate_midpoint(\n    row['latitude'], row['longitude'],\n    row['end_latitude'], row['end_longitude']\n)\n</code></pre>"},{"location":"architecture/transformation_engine/#wkt-geometries","title":"WKT Geometries","text":"<pre><code># Create LINESTRING for tow track\nfootprint_wkt = (\n    f\"LINESTRING (\"\n    f\"{row['longitude']} {row['latitude']}, \"\n    f\"{row['end_longitude']} {row['end_latitude']}\"\n    f\")\"\n)\n</code></pre>"},{"location":"architecture/transformation_engine/#multi-source-enrichment","title":"Multi-Source Enrichment","text":"<p>Challenge: Join catch data with species lookup to get taxonomic info.</p> <p>Solution: DataFrame merge + field extraction:</p> <pre><code>def transform_to_occurrence(self, catch_df, species_df):\n    # Join on species_common_name\n    merged = catch_df.merge(\n        species_df,\n        on='species_common_name',\n        how='left'\n    )\n\n    # Now merged has both catch data AND taxonomic data\n    for _, row in merged.iterrows():\n        occurrence = {\n            'scientificName': row['species_scientific_name'],  # From species_df\n            'scientificNameID': self.format_itis_lsid(row['ITIS_tsn']),  # From species_df\n            'vernacularName': row['species_common_name'],  # From catch_df\n            'individualCount': row['total_count'],  # From catch_df\n            # ...\n        }\n</code></pre>"},{"location":"architecture/transformation_engine/#record-expansion-1many","title":"Record Expansion (1:Many)","text":"<p>Challenge: Generate multiple eMoF records from single catch record.</p> <p>Solution: Loop and append:</p> <pre><code>def transform_to_emof(self, catch_df):\n    emof_records = []\n\n    for _, row in catch_df.iterrows():\n        occurrence_id = self.create_occurrence_id(...)\n\n        # Generate multiple measurement records from one catch record:\n\n        # 1. Size class (if present)\n        if pd.notna(row.get('size_class')):\n            emof_records.append({\n                'measurementType': 'size class',\n                'measurementValue': row['size_class'],\n                # ...\n            })\n\n        # 2. Total weight\n        if pd.notna(row.get('total_weight')):\n            emof_records.append({\n                'measurementType': 'total biomass',\n                'measurementValue': row['total_weight'],\n                'measurementUnit': 'kg',\n                # ...\n            })\n\n        # 3. Total count\n        # 4. Mean length\n        # 5. Std dev length\n        # (similar pattern)\n\n    return pd.DataFrame(emof_records)\n</code></pre> <p>Result: One catch record \u2192 up to 5 eMoF records.</p>"},{"location":"architecture/transformation_engine/#hybrid-integration","title":"Hybrid Integration","text":"<p>The two stages work together:</p> <pre><code># In DwCTransformer.transform_to_occurrence():\n\n# Step 1: Auto-rename via MappingEngine\nif self.mapping_engine:\n    auto_renamed = self.mapping_engine.transform_dataframe(\n        merged, \n        \"Occurrence\"\n    )\n\n# Step 2: Custom logic for complex fields\noccurrences = []\nfor _, row in merged.iterrows():\n    occurrence = {\n        'occurrenceID': self.create_occurrence_id(...),  # Custom\n        'eventID': self.create_event_id(...),            # Custom\n        'basisOfRecord': 'HumanObservation',             # Static\n        'scientificNameID': self.format_itis_lsid(...),  # Custom\n        # ...\n    }\n    occurrences.append(occurrence)\n\nresult_df = pd.DataFrame(occurrences)\n\n# Step 3: Merge - custom fields take precedence\nfor col in auto_renamed.columns:\n    if col not in result_df.columns:\n        result_df[col] = auto_renamed[col]\n\nreturn result_df\n</code></pre>"},{"location":"architecture/transformation_engine/#design-principles","title":"Design Principles","text":""},{"location":"architecture/transformation_engine/#separation-of-concerns","title":"Separation of Concerns","text":"<ul> <li>MappingEngine: Simple, reusable field renaming</li> <li>DwCTransformer: Domain-specific business logic</li> </ul>"},{"location":"architecture/transformation_engine/#explicit-implicit","title":"Explicit &gt; Implicit","text":"<ul> <li>Complex transformations require explicit code</li> <li>No \"magic\" - transformations are traceable</li> <li>Mappings document what happens, code implements how</li> </ul>"},{"location":"architecture/transformation_engine/#fail-safe-defaults","title":"Fail-Safe Defaults","text":"<pre><code># MappingEngine skips unknown fields rather than erroring\nif source_field not in source_df.columns:\n    if mapping_spec['required']:\n        warnings.warn(f\"Required field missing: {source_field}\")\n    continue  # Skip, don't crash\n\n# DwCTransformer checks for null values\nif pd.isna(row.get('size_class')):\n    # Don't create size_class eMoF record\n    pass\n</code></pre>"},{"location":"architecture/transformation_engine/#type-safety","title":"Type Safety","text":"<pre><code># Always convert types explicitly\n'individualCount': int(row['total_count']) if pd.notna(row['total_count']) else None\n\n# Round coordinates to appropriate precision\n'decimalLatitude': round(mid_lat, 6)\n</code></pre>"},{"location":"architecture/transformation_engine/#reusability","title":"Reusability","text":"<p>The MappingEngine works with any data following the LinkML pattern:</p> <pre><code># Works for OW1 data\nengine = MappingEngine('ow1-to-dwc-mappings.yaml')\nevent_df = engine.transform_dataframe(tow_data, 'Event')\n\n# Works for different survey with same pattern\nengine2 = MappingEngine('other-survey-to-dwc-mappings.yaml')\nevent_df2 = engine2.transform_dataframe(other_survey_data, 'Event')\n</code></pre> <p>Requirements:</p> <ol> <li>Source data documented in LinkML schema</li> <li>Mapping schema with <code>exact_mappings</code></li> <li>Custom transformer for complex logic (if needed)</li> </ol>"},{"location":"architecture/transformation_engine/#performance-considerations","title":"Performance Considerations","text":"<p>For large datasets:</p> <ul> <li>Vectorized operations where possible (pandas apply)</li> <li>Batch processing for ID generation</li> <li>Lazy evaluation - only transform needed fields</li> <li>Memory-efficient - no unnecessary copies</li> </ul>"},{"location":"architecture/transformation_engine/#next-steps","title":"Next Steps","text":"<ul> <li>View Complete Workflow</li> <li>Explore Reusability Guide</li> <li>See Example Outputs</li> </ul>"},{"location":"schemas/dwc-mappings/","title":"Darwin Core Archive Publication Model for OW1 Catch Data","text":"<p>Darwin Core Archive structure defining how Rutgers OW1 Bottom Trawl Survey catch data should be published as a DwC-A. This schema defines Event core, Occurrence extension, and ExtendedMeasurementOrFact (eMoF) extension classes with mappings back to source OW1 fields. Uses SKOS mapping predicates in reverse (DwC terms map to OW1 fields) to document both the semantic relationships and the publishing structure.</p> <p>Schema file: <code>models/datasets/rutgers/ow1-to-dwc-mappings.yaml</code></p>"},{"location":"schemas/dwc-mappings/#mapping-overview","title":"Mapping Overview","text":"<pre><code>flowchart LR\n    A[Source Fields] --&gt;|exact_mappings| B[Target Terms]\n    A --&gt;|related_mappings| C[Custom Transform]\n    C --&gt; B\n\n    style A fill:#e1f5ff\n    style B fill:#d4edda\n    style C fill:#fff4e1</code></pre> <p>Mapping types:</p> <ul> <li>exact_mappings: 1:1 field renames (auto-transformed)</li> <li>related_mappings: Complex transformations requiring custom logic</li> <li>close_mappings: Conceptually similar fields</li> </ul>"},{"location":"schemas/dwc-mappings/#darwincorearchive-mappings","title":"DarwinCoreArchive Mappings","text":"<p>Container representing a complete Darwin Core Archive for OW1 catch data.</p>"},{"location":"schemas/dwc-mappings/#event-mappings","title":"Event Mappings","text":"<p>Event core records representing sampling events in the OW1 survey. Hierarchical structure: cruise (parent) \u2192 tow (child).</p>"},{"location":"schemas/dwc-mappings/#auto-mapped-fields-11","title":"Auto-Mapped Fields (1:1)","text":"Target Term Source Field Transformation parentEventID <code>tow_cruise</code> Direct copy eventDate <code>tow_time</code> Direct copy locationID <code>station</code> Direct copy"},{"location":"schemas/dwc-mappings/#custom-mapped-fields","title":"Custom-Mapped Fields","text":"Target Term Source Fields Transformation eventID <code>tow_cruise</code>, <code>tow_station</code> Combine TowRecord.tow_cruise + TowRecord.tow_station to create unique eventID decimalLatitude <code>start_latitude</code>, <code>end_latitude</code> Calculate midpoint from TowRecord.start_latitude and TowRecord.end_latitude decimalLongitude <code>start_longitude</code>, <code>end_longitude</code> Calculate midpoint from TowRecord.start_longitude and TowRecord.end_longitude samplingProtocol <code>length_type</code> Document from dataset metadata: bottom trawl, 3 knots, 20 minutes"},{"location":"schemas/dwc-mappings/#occurrence-mappings","title":"Occurrence Mappings","text":"<p>Occurrence extension records representing species observations within sampling events. Each occurrence represents a single species (or species+size_class) caught during a tow.</p>"},{"location":"schemas/dwc-mappings/#auto-mapped-fields-11_1","title":"Auto-Mapped Fields (1:1)","text":"Target Term Source Field Transformation vernacularName <code>species_common_name</code> Direct copy scientificName <code>species_scientific_name_lookup</code> Direct copy"},{"location":"schemas/dwc-mappings/#custom-mapped-fields_1","title":"Custom-Mapped Fields","text":"Target Term Source Fields Transformation occurrenceID <code>index</code> Can use OW1 index as base for occurrenceID eventID <code>tow_cruise</code>, <code>tow_station</code> Combine TowRecord.tow_cruise + TowRecord.tow_station to create unique eventID occurrenceRemarks <code>size_class</code> Include size_class information if present"},{"location":"schemas/dwc-mappings/#extendedmeasurementorfact-mappings","title":"ExtendedMeasurementOrFact Mappings","text":"<p>ExtendedMeasurementOrFact (eMoF) extension records for measurements and observations. Used for length measurements, weights, counts, and size class designations.</p>"},{"location":"schemas/dwc-mappings/#custom-mapped-fields_2","title":"Custom-Mapped Fields","text":"Target Term Source Fields Transformation occurrenceID <code>index</code> Can use OW1 index as base for occurrenceID eventID <code>tow_cruise</code>, <code>tow_station</code> Combine TowRecord.tow_cruise + TowRecord.tow_station to create unique eventID measurementType <code>length_type</code> Examples: 'mean length', 'length standard deviation', 'total weight', 'total count' measurementValue <code>mean_length</code>, <code>std_length</code>, <code>total_weight</code>, <code>total_count</code> Direct value from source measurement field measurementRemarks <code>length_type</code> Document length measurement type (FL, TL, DW, CW, ML, SW, SH)"},{"location":"schemas/eml-mappings/","title":"EML Publication Model for OW1 Dataset Metadata","text":"<p>Ecological Metadata Language (EML) structure defining how Rutgers OW1 Bottom Trawl Survey dataset-level metadata should be published. This schema defines EML elements with mappings back to source OW1 DatasetMetadata fields. Uses SKOS mapping predicates in reverse (EML elements map to OW1 fields) to document both semantic relationships and publishing structure.</p> <p>Schema file: <code>models/datasets/rutgers/ow1-to-eml-mappings.yaml</code></p>"},{"location":"schemas/eml-mappings/#purpose","title":"Purpose","text":"<p>EML provides standardized dataset-level metadata for the Darwin Core Archive.</p>"},{"location":"schemas/eml-mappings/#metadata-mappings","title":"Metadata Mappings","text":""},{"location":"schemas/eml-mappings/#responsibleparty","title":"ResponsibleParty","text":"<p>Information about a person or organization responsible for the dataset. Used for creator, contact, and associatedParty roles.</p> EML Element Source Field Notes individual_name Generated organization_name Generated position_name Generated address Generated phone Generated electronic_mail_address Generated online_url Generated role <code>contributor_role</code> Map from contributors list with roles"},{"location":"schemas/eml-mappings/#project","title":"Project","text":"<p>Information about the research project that produced the dataset.</p> EML Element Source Field Notes title <code>dataset_title</code> Direct mapping from dataset_title personnel <code>contributors</code> Map from contributors list abstract <code>summary</code> Direct mapping from summary field funding <code>acknowledgement</code> Direct mapping from acknowledgement field"},{"location":"schemas/eml-mappings/#coverage","title":"Coverage","text":"<p>Information about the geographic, temporal, and taxonomic coverage of the dataset.</p> EML Element Source Field Notes geographic_coverage Generated temporal_coverage Generated Requires temporal extent data from station/tow tables taxonomic_coverage <code>species_common_name</code>, <code>species_scientific_name_lookup</code>, <code>itis_tsn</code> Aggregate unique species from all CatchRecords"},{"location":"schemas/eml-mappings/#geographiccoverage","title":"GeographicCoverage","text":"<p>Geographic area(s) covered by the dataset.</p> EML Element Source Field Notes geographic_description Generated Describe survey area: OW1 Wind Farm lease area and control sites bounding_coordinates Generated Requires coordinate data from station/tow tables"},{"location":"schemas/eml-mappings/#boundingcoordinates","title":"BoundingCoordinates","text":"<p>Geographic bounding box defined by coordinates.</p> EML Element Source Field Notes west_bounding_coordinate Generated east_bounding_coordinate Generated north_bounding_coordinate Generated south_bounding_coordinate Generated"},{"location":"schemas/eml-mappings/#taxonomiccoverage","title":"TaxonomicCoverage","text":"<p>General taxonomic scope of the dataset, describing the major taxonomic groups covered.</p> EML Element Source Field Notes general_taxonomic_coverage Generated Example: 'Bottom-dwelling fish and invertebrates from the Mid-Atlantic Bight'"},{"location":"schemas/eml-mappings/#methods","title":"Methods","text":"<p>Information about the methods used to collect or process the data.</p> EML Element Source Field Notes method_step <code>comment</code> Document sampling methodology from comment field sampling Generated Describe: 20 tows in lease area, 20 in control area per season quality_control Generated Document QA/QC procedures if available"},{"location":"schemas/eml-mappings/#distribution","title":"Distribution","text":"<p>Information about how the dataset is distributed or accessed.</p> EML Element Source Field Notes online Generated"},{"location":"schemas/eml-mappings/#online","title":"Online","text":"<p>Information about online access to the dataset.</p> EML Element Source Field Notes online_url Generated online_description Generated Describe ERDDAP access or other distribution methods"},{"location":"schemas/source-data/","title":"Rutgers OW1 Bottom Trawl Survey - Catch Data","text":"<p>Fisheries Monitoring in Ocean Wind 01. Two seasonal pre-construction surveys. Twenty tows conducted in the lease area and twenty in the control area each season (160/year total). Tows conducted at three knots for 20 minutes. Trawl data will inform relative biomass (kg/tow) by species, size frequency by species, condition index (length-weight) by species, community assemblage, and gut content of a subset of species.</p> <p>Schema file: <code>models/datasets/rutgers/ow1-catch-schema.yaml</code></p>"},{"location":"schemas/source-data/#data-fields-slots","title":"Data Fields (Slots)","text":""},{"location":"schemas/source-data/#datasetmetadata-fields","title":"DatasetMetadata Fields","text":"<p>Metadata describing the dataset as a whole, including provenance, institutional information, and project details.</p> Field Type Units Description ERDDAP Source acknowledgement string - Funding and acknowledgement information <code>NC_GLOBAL.acknowledgement</code> cdm_data_type string - Climate and Forecast (CF) / COARDS data type <code>NC_GLOBAL.cdm_data_type</code> comment string - General comments about the survey <code>NC_GLOBAL.comment</code> contributors Contributor - List of people and organizations that contributed to the dataset <code>NC_GLOBAL.contributor_name, NC_GLOBAL.contributor_role</code> conventions string - Metadata conventions followed by this dataset <code>NC_GLOBAL.Conventions</code> creator_email string - Email address of the dataset creator <code>NC_GLOBAL.creator_email</code> creator_institution string - Institution of the dataset creator <code>NC_GLOBAL.creator_institution</code> creator_name string - Name of the dataset creator <code>NC_GLOBAL.creator_name</code> creator_type string - Type of creator (e.g., person, group, organization) <code>NC_GLOBAL.creator_type</code> creator_url string - URL for the creator or creator's institution <code>NC_GLOBAL.creator_url</code> geospatial_bounds_crs string - Coordinate reference system for geospatial bounds <code>NC_GLOBAL.geospatial_bounds_crs</code> geospatial_vertical_positive string - Direction of positive vertical coordinates <code>NC_GLOBAL.geospatial_vertical_positive</code> dataset_id string - Unique identifier for the dataset <code>NC_GLOBAL.id</code> info_url string - URL for more information about the dataset or project <code>NC_GLOBAL.infoUrl</code> institution string - Institution responsible for the dataset <code>NC_GLOBAL.institution</code> keywords string - Keywords describing the dataset content <code>NC_GLOBAL.keywords</code> license string - License or data use agreement for the dataset <code>NC_GLOBAL.license</code> platform_name string - Name of the research vessel or platform used for data collection <code>NC_GLOBAL.platform_name</code> program string - Research program under which data was collected <code>NC_GLOBAL.program</code> project string - Specific project or projects associated with the dataset <code>NC_GLOBAL.project</code> publisher_email string - Email address of the dataset publisher <code>NC_GLOBAL.publisher_email</code> publisher_institution string - Institution of the dataset publisher <code>NC_GLOBAL.publisher_institution</code> publisher_name string - Name of the dataset publisher <code>NC_GLOBAL.publisher_name</code> publisher_type string - Type of publisher (e.g., person, group, organization) <code>NC_GLOBAL.publisher_type</code> source_url string - Source URL for the data files <code>NC_GLOBAL.sourceUrl</code> standard_name_vocabulary string - Vocabulary standard used for variable names <code>NC_GLOBAL.standard_name_vocabulary</code> summary string - Summary description of the dataset <code>NC_GLOBAL.summary</code> dataset_title string - Title of the dataset <code>NC_GLOBAL.title</code>"},{"location":"schemas/source-data/#contributor-fields","title":"Contributor Fields","text":"<p>A person or organization that contributed to the dataset.</p> Field Type Units Description ERDDAP Source contributor_name string - Name of the contributor <code>NC_GLOBAL.contributor_name</code> contributor_role string - Role of the contributor in the project <code>NC_GLOBAL.contributor_role</code>"},{"location":"schemas/source-data/#towrecord-fields","title":"TowRecord Fields","text":"<p>A record representing a single trawl tow/station event. ERDDAP dataset: bottom_trawl_survey_ow1_tows</p> Field Type Units Description ERDDAP Source index integer - Unique sequential identifier for each record <code>index</code> time string - Date and time of the tow in ISO 8601 format <code>time</code> cruise string - Unique identifier for the survey cruise <code>cruise</code> station string - Station identifier. C/I prefix designates the type of station: Control (C) - open sand bottom without turbine placement, Impact (I) - OW1 Wind Farm lease area. <code>station</code> latitude float deg Latitude coordinates entered at start of trawl <code>latitude (start_latitude)</code> end_latitude float deg Latitude coordinates entered at end of trawl <code>end_latitude</code> longitude float deg Longitude coordinates entered at start of trawl <code>longitude (start_longitude)</code> end_longitude float deg Longitude coordinates entered at end of trawl <code>end_longitude</code>"},{"location":"schemas/source-data/#catchrecord-fields","title":"CatchRecord Fields","text":"<p>A record representing catch data from a single species (or species+size_class combination) collected during a bottom trawl survey tow/station. ERDDAP dataset: bottom_trawl_survey_ow1_catch</p> Field Type Units Description ERDDAP Source index integer - Unique sequential identifier for each record <code>index</code> cruise string - Unique identifier for the survey cruise <code>cruise</code> station string - Station identifier. C/I prefix designates the type of station: Control (C) - open sand bottom without turbine placement, Impact (I) - OW1 Wind Farm lease area. <code>station</code> species_common_name string - Common name of the species caught <code>species_common_name</code> size_class string - Size class designation when distinct/noticeable size differences exist within the same species in a catch. Used to avoid biasing randomly selected length measurements. <code>size_class</code> total_weight float kg Total weight in kilograms of measured individuals for this species/size_class combination. <code>total_weight</code> total_count integer - Total count of individuals in this species/size_class combination <code>total_count</code> length_type LengthTypeEnum - Type of length measurement used for this species <code>length_type</code> mean_length float mm Mean length of measured individuals <code>mean_length</code> std_length float mm Standard deviation of length measurements <code>std_length</code>"},{"location":"schemas/source-data/#speciescode-fields","title":"SpeciesCode Fields","text":"<p>Taxonomic lookup table linking common names to scientific names and ITIS TSNs. ERDDAP dataset: species_id_codes</p> Field Type Units Description ERDDAP Source speciesID float - Numeric identifier for species based on NOAA National Marine Fisheries Service codes <code>speciesID</code> species_scientific_name string - Scientific name of the species <code>species_scientific_name</code> ITIS_tsn float - Integrated Taxonomic Information System (ITIS) taxonomic serial number <code>ITIS_tsn</code> species_common_name string - Common name of the species caught <code>species_common_name</code>"},{"location":"schemas/source-data/#enumerations","title":"Enumerations","text":""},{"location":"schemas/source-data/#lengthtypeenum","title":"LengthTypeEnum","text":"<p>Standardized length measurement types used in trawl surveys</p> Code Meaning Description FL FL Fork Length TL TL Total Length DW DW Disk Width (for large stingrays) CW CW Carapace Width (for crabs, horseshoe crabs, and species with a carapace) ML ML Mantle Length (for squid, measuring just the length of the mantle tissue) SW SW Shell Width (for general bivalves and clams) SH SH Shell Height (for scallops)"},{"location":"schemas/source-data/#data-classes","title":"Data Classes","text":"<p>The schema organizes fields into classes:</p>"},{"location":"schemas/source-data/#datasetmetadata","title":"DatasetMetadata","text":"<p>Metadata describing the dataset as a whole, including provenance, institutional information, and project details.</p> <p>Key fields: acknowledgement, cdm_data_type, comment, contributors, conventions</p>"},{"location":"schemas/source-data/#contributor","title":"Contributor","text":"<p>A person or organization that contributed to the dataset.</p> <p>Key fields: contributor_name, contributor_role</p>"},{"location":"schemas/source-data/#towrecord","title":"TowRecord","text":"<p>A record representing a single trawl tow/station event. ERDDAP dataset: bottom_trawl_survey_ow1_tows</p> <p>Key fields: index, time, cruise, station, latitude</p>"},{"location":"schemas/source-data/#catchrecord","title":"CatchRecord","text":"<p>A record representing catch data from a single species (or species+size_class combination) collected during a bottom trawl survey tow/station. ERDDAP dataset: bottom_trawl_survey_ow1_catch</p> <p>Key fields: index, cruise, station, species_common_name, size_class</p>"},{"location":"schemas/source-data/#speciescode","title":"SpeciesCode","text":"<p>Taxonomic lookup table linking common names to scientific names and ITIS TSNs. ERDDAP dataset: species_id_codes</p> <p>Key fields: speciesID, species_scientific_name, ITIS_tsn, species_common_name</p>"}]}